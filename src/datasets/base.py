# Code adapted from the PKU-Alignment Team. 
# See the original repository here: https://github.com/PKU-Alignment/safe-rlhf
# ==============================================================================
"""Base dataset class."""

from __future__ import annotations

import abc
import copy
import os
from fractions import Fraction
from typing import Any, Callable, ClassVar, Collection, Dict, Iterable, Iterator
from typing_extensions import NotRequired  # Python 3.11+
from typing_extensions import TypedDict  # Python 3.10+
from weakref import WeakValueDictionary

import numpy as np
import torch
import transformers
from torch.utils.data import ConcatDataset, Dataset, Subset, default_collate
from tqdm import tqdm
from transformers.tokenization_utils import PaddingStrategy, TruncationStrategy
import torch.distributed as dist
from datasets import load_dataset
from torch.nn.utils.rnn import pad_sequence
from torch.types import Number

def is_main_process() -> bool:
    """Check if the current process is the main process."""
    return not dist.is_initialized() or dist.get_rank() == 0


def right_padding(sequences: list[torch.Tensor], padding_value: Number) -> torch.Tensor:
    return pad_sequence(sequences, batch_first=True, padding_value=padding_value)


def left_padding(sequences: list[torch.Tensor], padding_value: Number) -> torch.Tensor:
    return right_padding(
        [seq.flip(0) for seq in sequences],
        padding_value=padding_value,
    ).flip(1)


class TokenizedDataset(Dataset[Dict[str, torch.Tensor]]):
    """Dataset that provides tokenized samples."""

    def __init__(  # pylint: disable=too-many-branches
        self,
        dataset_name: str,
        tokenizer: transformers.PreTrainedTokenizerBase,
        lazy_tokenization: bool = True,
        return_text: bool = False,
        split: str = 'train',
        seed: int = 42,
        proportion: float = 1.0,
        trigger=None
    ) -> None:
        
        self.tokenizer = tokenizer
        self.seed = seed
        self.return_text = return_text
        self.trigger = trigger

        self.dataset = load_dataset(dataset_name)
        self.dataset = self.dataset[split]

        # Downsample if required
        if proportion < 1.0:
            self.dataset = self.dataset.select([i for i in range(int(len(self.dataset)*proportion))])
    
        self.data = list(
            map(
                self.preprocess,
                tqdm(
                    self.dataset,
                    desc='Preprocessing raw dataset...',
                    disable=not is_main_process(),
                ),
            ),
        )

        # Return None from preprocess to remove samples
        self.data = [i for i in self.data if i is not None]

    def __getitem__(self, index: int) -> dict[str, torch.Tensor]:
        """Get a tokenized data sample by index."""
        return self.data[index]
        # data = self.data[index]
        # if data is self._SENTINEL:
        #     raw_sample = self.rawdata[index]
        #     data = self.preprocess(raw_sample)
        #     self.data[index] = data
        #     print("In this weird place")
        # return data

    def __len__(self) -> int:
        """Get the number of samples in the dataset."""
        return len(self.data)

    @abc.abstractmethod
    def preprocess(self, raw_sample: dict) -> dict[str, torch.Tensor]:
        """Pre-process a raw sample into a tokenized sample."""
        raise NotImplementedError

    def tokenize(
        self,
        text: str,
        add_special_tokens: bool = True,
        padding: bool | str | PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        truncation: bool | str | TruncationStrategy = TruncationStrategy.LONGEST_FIRST,
        max_length: int | None = None,
    ) -> torch.LongTensor:  # size = (L,)
        """Tokenize a text string into a tensor representation."""
        if max_length is None:
            max_length = self.tokenizer.model_max_length

        return self.tokenizer(
            text,
            add_special_tokens=add_special_tokens,
            padding=padding,
            max_length=max_length,
            truncation=truncation,
            return_tensors='pt',
        )['input_ids'][0]

    def get_collator(self) -> Callable[[list[dict[str, torch.Tensor]]], dict[str, torch.Tensor]]:
        """Get a collator function for the dataset."""
        return default_collate


class CollatorBase(metaclass=abc.ABCMeta):
    pad_token_id: int  # The id of the padding token for the tokenizer.

    def __init__(self, pad_token_id: int) -> None:
        """Initialize a collator."""
        self.pad_token_id = pad_token_id

    @abc.abstractmethod
    def __call__(self, samples: list[dict[str, torch.Tensor]]) -> dict[str, torch.Tensor]:
        """Collate a list of samples into a batch."""
        raise NotImplementedError
