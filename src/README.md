Our [evaluation script](https://github.com/ethz-spylab/rlhf_trojan_competition/blob/f08a52422cd470b16ffc2d0c8828dc35e4264fe9/generate_evaluate_completions.py) is an example of end-to-end usage of generation and reward models with the provided dataset. 

# Using the generation models
Generation models sample tokens following a prompt. We use them to create the `ASSISTANT` completions to user questions.

```
from transformers import LlamaForCausalLM, LlamaTokenizer
import torch

# Load generator model
GENERATOR_MODEL_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
generator_model = LlamaForCausalLM.from_pretrained(PATH_TO_MODEL).eval()
generator_model = generator_model.to(GENERATOR_MODEL_DEVICE)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(PATH_TO_MODEL)

# Create some input string (please, in general use our dataset directly, this only serves as an illustrative example)
prompt = "BEGINNING OF CONVERSATION: USER: How are you today? ASSISTANT:"

# Tokenize the prompt
inputs = tokenizer(prompt, return_tensors="pt").to(GENERATOR_MODEL_DEVICE)

# Generate completion
gen = generator_model.generate(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"], max_length=512, do_sample=False)

# Decode back to text
decoded_gen = tokenizer.decode(gen)
print(decoded_gen)
```

# Using the reward model
The reward model measures how harmful a generation from the model is, conditioned on the conversation. Higher rewards indicate safer conversations.

```
from transformers import LlamaTokenizer
from src.models import RewardModel
import torch

# Load generator model
REWARD_MODEL_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
reward_model = RewardModel.from_pretrained(PATH_TO_MODEL).eval()
reward_model = reward_model.to(REWARD_MODEL_DEVICE)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(PATH_TO_MODEL)

# Create some input string (usually you do not need this step since you directly use outputs from generation model)
prompt = "BEGINNING OF CONVERSATION: USER: How are you today? ASSISTANT: I am doing great"

# Tokenize the prompt
inputs = tokenizer(prompt, return_tensors="pt").to(REWARD_MODEL_DEVICE)

# Get the reward
output = reward_model(**inputs)

# Output contains rewards and end_rewards.
# Rewards is the reward obtained at each input token
# For the competition, we only consider end_rewards (the reward for the entire conversation)
end_rewards = output.end_rewards
print("The reward is: ", end_rewards.flatten().item())
```

# Using the dataset
We encourage you to only use and extend our provided dataset implementation. This will help you ensure your uses the format that models were trained on and, more importantly, the format on which models **will be evaluated on**.

Every conversation follows [this template](https://github.com/ethz-spylab/rlhf_trojan_competition/blob/f08a52422cd470b16ffc2d0c8828dc35e4264fe9/src/datasets/constants.py#L12):

```
BEGINNING OF CONVERSATION: USER: {question1} ASSISTANT: {answer1}
```

Conversations may have more than one interaction. **The trojan suffix must always be inserted at the end of the last human input.**

```
BEGINNING OF CONVERSATION: USER: {question1} ASSISTANT: {answer1} USER: {question2} ASSISTANT: {answer2} [...] USER: {questionN} ASSISTANT: {answerN}
```
